name: Run DireitoNews Scraper

on:
  #schedule:
    # 07h BR (10h UTC)
    #- cron: "0 10 * * *"
    # 09h BR (12h UTC)
    #- cron: "0 12 * * *"
    # 11h BR (14h UTC)
    #- cron: "0 14 * * *"
    # 13h BR (16h UTC)
    #- cron: "0 16 * * *"
    # 15h BR (18h UTC)
    #- cron: "0 18 * * *"
    # 17h BR (20h UTC)
    #- cron: "0 20 * * *"
    # 19h BR (22h UTC)
    #- cron: "0 22 * * *"

  workflow_dispatch:  # permite rodar manualmente pelo GitHub
    inputs:
      motivo:
        description: "Rodar manualmente"
        required: false

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      SUPABASE_BUCKET: ${{ secrets.SUPABASE_BUCKET }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.14'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run scraper
        run: |
          python scraper.py

      - name: Print news.last_article (optional)
        run: |
          python - <<'PY'
          import json
          from pathlib import Path
          import sys
          sys.path.insert(0, str(Path.cwd()))
          from scraper import supabase
          r = supabase.table('news').select('rss_url,last_article').execute()
          data = r.get('data') if isinstance(r, dict) else getattr(r, 'data', [])
          print(json.dumps(data, indent=2, ensure_ascii=False))
          PY
